# Neural Network Matrix Shapes (Forward + Backpropagation)

This document uses the notation:
- M âˆˆ â„^{rÃ—c}       : Matrix M is real-valued with r rows and c columns
- â„                : Real numbers
- b                : Batch size (number of inputs processed at once)
- m                : Number of inputs into the current layer (i.e. output size of previous layer)
- n                : Number of neurons in the current layer
- náµ¢               : Number of neurons in layer i
- L                : Total number of layers (excluding input)
- âŠ™                : Element-wise multiplication

---

## ğŸ” Forward Propagation

Assume a 3-layer network:
Input (nâ‚€ = 2) â†’ Hidden1 (nâ‚ = 5) â†’ Hidden2 (nâ‚‚ = 3) â†’ Output (nâ‚ƒ = 1)

### Layer 1 (Input â†’ Hidden1)
- Input batch:         `X âˆˆ â„^{bÃ—nâ‚€}`
- Weights:             `Wâ‚ âˆˆ â„^{nâ‚€Ã—nâ‚}` â† (m = nâ‚€, n = nâ‚)
- Bias:                `bâ‚ âˆˆ â„^{1Ã—nâ‚}`
- Pre-activation:      `Zâ‚ = XÂ·Wâ‚ + bâ‚ âˆˆ â„^{bÃ—nâ‚}`
- Activation output:   `Aâ‚ = f(Zâ‚) âˆˆ â„^{bÃ—nâ‚}`

### Layer 2 (Hidden1 â†’ Hidden2)
- Weights:             `Wâ‚‚ âˆˆ â„^{nâ‚Ã—nâ‚‚}` â† (m = nâ‚, n = nâ‚‚)
- Bias:                `bâ‚‚ âˆˆ â„^{1Ã—nâ‚‚}`
- Pre-activation:      `Zâ‚‚ = Aâ‚Â·Wâ‚‚ + bâ‚‚ âˆˆ â„^{bÃ—nâ‚‚}`
- Activation output:   `Aâ‚‚ = f(Zâ‚‚) âˆˆ â„^{bÃ—nâ‚‚}`

### Layer 3 (Hidden2 â†’ Output)
- Weights:             `Wâ‚ƒ âˆˆ â„^{nâ‚‚Ã—nâ‚ƒ}` â† (m = nâ‚‚, n = nâ‚ƒ)
- Bias:                `bâ‚ƒ âˆˆ â„^{1Ã—nâ‚ƒ}`
- Pre-activation:      `Zâ‚ƒ = Aâ‚‚Â·Wâ‚ƒ + bâ‚ƒ âˆˆ â„^{bÃ—nâ‚ƒ}`
- Activation output:   `Aâ‚ƒ = Ïƒ(Zâ‚ƒ) âˆˆ â„^{bÃ—nâ‚ƒ}`

- Predictions:         `Å· âˆˆ â„^{bÃ—nâ‚ƒ}` (for binary: nâ‚ƒ = 1)
- Ground truth labels: `y âˆˆ â„^{bÃ—nâ‚ƒ}`

---

## ğŸ” Backpropagation

### Output Layer (Layer 3)
- `dAâ‚ƒ = âˆ‚L/âˆ‚Aâ‚ƒ âˆˆ â„^{bÃ—nâ‚ƒ}`
- `dZâ‚ƒ = dAâ‚ƒ âŠ™ Ïƒâ€²(Zâ‚ƒ) âˆˆ â„^{bÃ—nâ‚ƒ}`
- `dWâ‚ƒ = Aâ‚‚áµ€ Â· dZâ‚ƒ âˆˆ â„^{nâ‚‚Ã—nâ‚ƒ}` â† (m = nâ‚‚, n = nâ‚ƒ)
- `dbâ‚ƒ = sum_rows(dZâ‚ƒ) âˆˆ â„^{1Ã—nâ‚ƒ}`
- `dAâ‚‚ = dZâ‚ƒ Â· Wâ‚ƒáµ€ âˆˆ â„^{bÃ—nâ‚‚}`

### Hidden Layer 2
- `dZâ‚‚ = dAâ‚‚ âŠ™ fâ€²(Zâ‚‚) âˆˆ â„^{bÃ—nâ‚‚}`
- `dWâ‚‚ = Aâ‚áµ€ Â· dZâ‚‚ âˆˆ â„^{nâ‚Ã—nâ‚‚}` â† (m = nâ‚, n = nâ‚‚)
- `dbâ‚‚ = sum_rows(dZâ‚‚) âˆˆ â„^{1Ã—nâ‚‚}`
- `dAâ‚ = dZâ‚‚ Â· Wâ‚‚áµ€ âˆˆ â„^{bÃ—nâ‚}`

### Hidden Layer 1
- `dZâ‚ = dAâ‚ âŠ™ fâ€²(Zâ‚) âˆˆ â„^{bÃ—nâ‚}`
- `dWâ‚ = Xáµ€ Â· dZâ‚ âˆˆ â„^{nâ‚€Ã—nâ‚}` â† (m = nâ‚€, n = nâ‚)
- `dbâ‚ = sum_rows(dZâ‚) âˆˆ â„^{1Ã—nâ‚}`

---

## ğŸ” Parameter Updates

For all layers `l âˆˆ {1, 2, 3}`:
W_l â† W_l - Î± Â· dW_l
b_l â† b_l - Î± Â· db_l

yaml
Copy
Edit
Where:
- Î± = learning rate
- Update shapes match original parameter shapes

---

## âœ… Summary Table

| Matrix        | Shape (Dim)         | Description                          |
|---------------|---------------------|--------------------------------------|
| X             | b Ã— nâ‚€              | Input features                       |
| Wâ‚            | nâ‚€ Ã— nâ‚             | Weights: Input â†’ Hidden1             |
| bâ‚            | 1 Ã— nâ‚              | Bias for Hidden1                     |
| Zâ‚, Aâ‚        | b Ã— nâ‚              | Pre/post activation of Hidden1       |
| Wâ‚‚            | nâ‚ Ã— nâ‚‚             | Weights: Hidden1 â†’ Hidden2           |
| bâ‚‚            | 1 Ã— nâ‚‚              | Bias for Hidden2                     |
| Zâ‚‚, Aâ‚‚        | b Ã— nâ‚‚              | Pre/post activation of Hidden2       |
| Wâ‚ƒ            | nâ‚‚ Ã— nâ‚ƒ             | Weights: Hidden2 â†’ Output            |
| bâ‚ƒ            | 1 Ã— nâ‚ƒ              | Bias for Output                      |
| Zâ‚ƒ, Aâ‚ƒ        | b Ã— nâ‚ƒ              | Final pre/post activation            |
| y, Å·          | b Ã— nâ‚ƒ              | Ground truth + prediction            |
| dZ, dW, db    | match forward dims  | Gradients for backpropagation        |

---

## ğŸ§  Notes
- **m** = input size into the layer (output size of previous layer)
- **n** = output size of the layer (number of neurons)
- All `dW` have shape `[mÃ—n]` like their respective `W`
- Broadcasting lets `b âˆˆ â„^{1Ã—n}` apply across batch
- You only need to keep track of shapes to debug flow and gradients
